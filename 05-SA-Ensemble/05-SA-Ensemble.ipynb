{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"05-SA-Ensemble.ipynb","provenance":[],"collapsed_sections":["Etu4iUlKyTvN","Q3NaBuOJeYBd","rAL2J-ReyqfT","wErv-k5yvYyg","OUqm7EcKzLTJ"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Yw0ov22IeEob"},"source":["## Clasificación de textos utilizando un **ensemble** de clasificadores\r\n","\r\n","La clasificación de textos consiste en, dado un texto, asignarle una entre varias categorías. Algunos ejemplos de esta tarea son:\r\n","\r\n","- dado un tweet, categorizar su connotación como positiva, negativa o neutra.\r\n","- dado un post de Facebook, clasificarlo como portador de un lenguaje ofensivo o no.  \r\n","\r\n","En la actividad exploraremos cómo implementar la técnica de **stacking** para combinar modelos y su aplicación para clasificar reviews de [IMDB](https://www.imdb.com/) sobre películas en las categorías \\[$positive$, $negative$\\]. \r\n","\r\n","\r\n","\r\n","Esta consiste en entrenar un modelo que realiza la clasificación a partir de las predicciones realizadas por otros clasificadores.\r\n","\r\n","Concretamente, combinaremos \r\n","un clasificador incluido en la librería [Transformers](https://huggingface.co/transformers/)  y un pipeline basado Support Vector Machines. Puede encontrar más información sobre los datos utilizados en [Kaggle](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) y en [Large Movie Review Datase](http://ai.stanford.edu/~amaas/data/sentiment/).\r\n","\r\n","**Instrucciones:**\r\n","\r\n","- siga las indicaciones y comentarios en cada apartado.\r\n","\r\n","\r\n","**Después de esta actividad nos habremos familiarizado con:**\r\n","- cómo combinar varios modelos para obtener un clasificador más robusto mediante **stacking**.\r\n","\r\n","- cómo contruir un pipeline para la clasificación de textos utilizando [scikit-learn](https://scikit-learn.org/stable/)..\r\n","\r\n","\r\n","- cómo instanciar un pipeline para la clasificación de textos utilizando la librería Transformers.\r\n","\r\n","**Requerimientos**\r\n","- python 3.6.12 - 3.8\r\n","- tensorflow==2.3.0\r\n","- transformers==4.2.1\r\n","- pandas==1.1.5\r\n","- plotly==4.13.0\r\n","- tqdm==4.56.0\r\n","- scikit-learn==0.24.0"]},{"cell_type":"markdown","metadata":{"id":"Etu4iUlKyTvN"},"source":["### Instalación de librerías e importación de dependencias.\r\n","\r\n","Para comenzar, es preciso instalar las dependencias, realizar los imports necesarios y definir algunas funciones auxiliares.\r\n","\r\n","Ejecute las siguientes casillas prestando atención a las instrucciones adicionales en los comentarios."]},{"cell_type":"code","metadata":{"id":"Ett8dNE9yTc2","executionInfo":{"status":"ok","timestamp":1615301679539,"user_tz":-60,"elapsed":7236,"user":{"displayName":"JOSE IGNACIO ABREU SALAS","photoUrl":"","userId":"06657112918169434086"}}},"source":["# instalar librerías. Esta casilla es últil por ejemplo si se ejecuta el cuaderno en Google Colab\r\n","# Note que existen otras dependencias como tensorflow==2.3.0, etc. que en este caso se encontrarían ya instaladas\r\n","%%capture\r\n","!pip install transformers==4.2.1\r\n","\r\n","print('Done!')"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z5JQo-p5eAAc","executionInfo":{"status":"ok","timestamp":1615305913977,"user_tz":-60,"elapsed":800,"user":{"displayName":"JOSE IGNACIO ABREU SALAS","photoUrl":"","userId":"06657112918169434086"}},"outputId":"8560ce4d-3b72-4fa2-9320-1b136fc1f55b"},"source":["# para establecer caminos al guardar y leer archivos\n","import os\n","\n","#  para construir gráficas y realizar análisis exploratorio de los datos\n","import plotly.graph_objects as go\n","from tqdm import tqdm\n","\n","# para cargar datos y realizar pre-procesamiento básico\n","import numpy as np\n","import pandas as pd\n","from collections import Counter\n","\n","# para pre-procesamiento del texto y extraer carácterísticas\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from nltk.stem.snowball import EnglishStemmer\n","from sklearn import preprocessing\n","\n","# algoritmos de clasificación\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.svm import SVC\n","\n","# para construir pipelines\n","from sklearn.pipeline import Pipeline\n","\n","# para evaluar los modelos \n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, plot_roc_curve\n","from sklearn.utils.multiclass import unique_labels\n","\n","# para guardar el modelo\n","import pickle\n","import tensorflow as tf\n","\n","# Comunes\n","\n","\n","# Modelo 01\n","\n","\n","# Modelo 02\n","\n","# algoritmos de clasificación, tokenizadores, etc.\n","from transformers import TextClassificationPipeline, DistilBertTokenizer, TFDistilBertForSequenceClassification, ModelCard\n","\n","from transformers.tokenization_utils import TruncationStrategy\n","\n","\n","# Modelo 03\n","# algoritmos de clasificación, tokenizadores, etc.\n","from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification, ModelCard, DistilBertConfig, TextClassificationPipeline\n","\n","\n","print('Done!')"],"execution_count":57,"outputs":[{"output_type":"stream","text":["Done!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8OZ02eoXC2Ex","executionInfo":{"status":"ok","timestamp":1615301686668,"user_tz":-60,"elapsed":14352,"user":{"displayName":"JOSE IGNACIO ABREU SALAS","photoUrl":"","userId":"06657112918169434086"}}},"source":["# evalua el pipeline entrenado de acuerdo a una de las métricas apropiadas para un problema de clasificación\r\n","def evaluate_model(model, X, y_true):\r\n","    y_pred = predict(model, X)\r\n","\r\n","    print('==== Sumario de la clasificación ==== ')\r\n","    print(classification_report(y_true, y_pred))\r\n","    \r\n","    print('Accuracy -> {:.2%}\\n'.format(accuracy_score(y_true, y_pred)))\r\n","    \r\n","    if hasattr(model, 'predict_proba'):\r\n","      y_scores = predict_proba(model, X)[:,1]\r\n","      rocs = roc_auc_score(y_true, y_scores)\r\n","      #rocc = roc_curve(y_true, y_score[:,1], pos_label='positive')\r\n","\r\n","      print('ROC Score ->  {:.2%}\\n'.format(rocs))\r\n","      plot_roc_curve(model, X, y_true)\r\n","      #print(rocc)\r\n","\r\n","    print('==== Matriz de confusión ==== ')\r\n","    cm = confusion_matrix(y_true, y_pred)\r\n","    display_labels = unique_labels(y_true, y_pred)\r\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=display_labels)\r\n","    disp.plot(include_values=True)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q3NaBuOJeYBd"},"source":["### Carga de datos y análisis exploratorio\r\n","\r\n","El primer paso consiste en obtener los datos relacionados con nuestra tarea dejándolos en el formato adecuado.  Existen diferentes opciones, entre estas:\r\n","\r\n","- montar nuestra partición de Google Drive y leer un fichero desde esta.\r\n","\r\n","- leer los datos desde un fichero en una carpeta local.\r\n","\r\n","- leer los datos directamente de un URL.\r\n","\r\n","En este caso, se encuentran en un fichero separado por comas con la siguiente estructura:\r\n","\r\n","| Phrase | Sentiment| \r\n","| ------ | ------ |\r\n","| This movie is really not all that bad...    | positive |\r\n","\r\n","\r\n","Ejecute la siguiente casilla para leer los datos."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d9GhhJ2PeW9C","executionInfo":{"status":"ok","timestamp":1615301687443,"user_tz":-60,"elapsed":15122,"user":{"displayName":"JOSE IGNACIO ABREU SALAS","photoUrl":"","userId":"06657112918169434086"}},"outputId":"d9ce64df-42e8-4699-f790-1ee3b50a0c03"},"source":["# descomente las siguientes 3 líneas para leer datos desde Google Drive,sumiendo que se trata de un fichero llamado review.csv localizado dentro de una carpeta llamada 'Datos' en su Google Drive\n","#from google.colab import drive\n","#drive.mount('/content/drive')\n","#path = '/content/drive/MyDrive/Datos/ejemplo_review_train.csv'\n","\n","\n","# descomente la siguiente línea para leer los datos desde un archivo local, por ejemplo, asumiendo que se encuentra dentro de un directorio llamado sample_data\n","#path = './sample_data/ejemplo_review_train.csv'\n","\n","\n","# descomente la siguiente línea para leer datos desde un URL\n","path = 'https://github.com/TeachingTextMining/TextClassification/raw/main/02-SA-Transformers-Basic/sample_data/ejemplo_review_train.csv'\n","\n","\n","# leer los datos\n","data = pd.read_csv(path, sep=',')\n","\n","print('Done!')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Done!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"N-ZyhoalLxd-"},"source":["Una vez leídos los datos, ejecute la siguiente casilla para construir una gráfica que muestra la distribución de clases en el corpus."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":539},"id":"lzRVAYtp033t","executionInfo":{"status":"ok","timestamp":1615301688556,"user_tz":-60,"elapsed":14438,"user":{"displayName":"JOSE IGNACIO ABREU SALAS","photoUrl":"","userId":"06657112918169434086"}},"outputId":"55fbdc76-f051-4889-f0d6-1b49a1beafc2"},"source":["# obtener algunas estadísticas sobre los datos\n","categories = sorted(data['Sentiment'].unique(), reverse=True)\n","hist= Counter(data['Sentiment']) \n","print('Total de instancias -> {0}'.format(data.shape[0]))\n","print('Distribución de clases -> {0}'.format({item[0]:round(item[1]/len(data['Sentiment']), 3) for item in sorted(hist.items(), key=lambda x: x[0])}))\n","\n","print('Categorías -> {0}'.format(categories))\n","print('Comentario de ejemplo -> {0}'.format(data['Phrase'][0]))\n","print('Categoría del comentario -> {0}'.format(data['Sentiment'][0]))\n","\n","colors = ['darkgreen', 'red']\n","fig = go.Figure(layout=go.Layout(height=400, width=600))\n","fig.add_trace(go.Bar(x=categories, y=[hist[cat] for cat in sorted(hist.keys())], marker_color=colors))\n","fig.show()\n","\n","print('Done!')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Total de instancias -> 1763\n","Distribución de clases -> {'negative': 0.511, 'positive': 0.489}\n","Categorías -> ['positive', 'negative']\n","Comentario de ejemplo -> This is a great movie that everyone should see. It plays like a Dean Koontz book.<br /><br />Bill Paxton's performance was great in that it really seems like he believes in what he is saying and doing.<br /><br />I don't know why viewers have to read in some kind of advocacy for religious murder in to the film. It is fiction. The ending is surprising, but fictional. So what? I think that is what makes this movie so good. SPOILER DO NOT READ FURTHER IF YOU HAVENT SEEN THE MOVIE. Throughout the movie, the viewer is continually shocked at the sickness of Paxton's character, the impact on the children, and the way the children handle this outrageous conduct. And then at the end, it turns out to be true. God has put him on a mission to rid the world of demons. Paxton is not clairvoyant as other viewers suggest. Sure, he is given info that he couldn't have known otherwise, but the movie goes further to show how God is \"protecting\" Adam through the convenient video quality problem and the complete lack of memory of the second FBI agent. The film isn't advocating Christian murder, it is merely taking the viewer on a very unexpected ride.\n","Categoría del comentario -> positive\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"3a5ed0a9-076e-4b0c-ae32-3ce215c873eb\" class=\"plotly-graph-div\" style=\"height:400px; width:600px;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"3a5ed0a9-076e-4b0c-ae32-3ce215c873eb\")) {\n","                    Plotly.newPlot(\n","                        '3a5ed0a9-076e-4b0c-ae32-3ce215c873eb',\n","                        [{\"marker\": {\"color\": [\"darkgreen\", \"red\"]}, \"type\": \"bar\", \"x\": [\"positive\", \"negative\"], \"y\": [901, 862]}],\n","                        {\"height\": 400, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"width\": 600},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('3a5ed0a9-076e-4b0c-ae32-3ce215c873eb');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Done!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vvxbQWzkvAB4"},"source":["Finalmente, ejecute la siguiente casilla para crear los conjuntos de entrenamiento y validación que se utilizarán para entrenar y validar los modelos."]},{"cell_type":"code","metadata":{"id":"BM8V-MykvAuy","executionInfo":{"status":"ok","timestamp":1615301688557,"user_tz":-60,"elapsed":12762,"user":{"displayName":"JOSE IGNACIO ABREU SALAS","photoUrl":"","userId":"06657112918169434086"}}},"source":["# obtener conjuntos de entrenamiento (90%) y validación (10%)\r\n","seed = 0    # fijar random_state para reproducibilidad\r\n","train, val = train_test_split(data, test_size=.1, stratify=data['Sentiment'], random_state=seed)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ILDWBeVa3_C6"},"source":["### Entrenamiento de los clasificadores base"]},{"cell_type":"markdown","metadata":{"id":"rAL2J-ReyqfT"},"source":["#### Modelo 1: Text Classification Pipeline\r\n","El primer modelo será un pipeline básico para la clasificación de textos.\r\n","\r\n","Ejecute la siguiente casilla para definir algunas variables y funciones auxiliares, instanciar el modelo y entrenarlo. Preste atención a las explicaciónes dadas en los comentarios."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"84qGD7FFmL_5","executionInfo":{"status":"ok","timestamp":1615301977659,"user_tz":-60,"elapsed":13872,"user":{"displayName":"JOSE IGNACIO ABREU SALAS","photoUrl":"","userId":"06657112918169434086"}},"outputId":"fef032d4-86df-472e-ab67-d528947796bc"},"source":["# listado de stopwords\r\n","stop_words=['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself','yourselves',\r\n","            'he','him','his','himself','she','her','hers','herself','it','its','itself','they','them','their',\r\n","            'theirs','themselves','what','which','who','whom','this','that','these','those','am','is','are',\r\n","            'was','were','be','been','being','have','has','had','having','do','does','did','doing','a','an',\r\n","            'the','and','but','if','or','because','as','until','while','of','at','by','for','with','about',\r\n","            'against','between','into','through','during','before','after','above','below','to','from','up',\r\n","            'down','in','out','on','off','over','under','again','further','then','once','here','there','when',\r\n","            'where','why','how','all','any','both','each','few','more','most','other','some','such','no','nor',\r\n","            'not','only','own','same','so','than','too','very','s','t','can','will','just','don','should','now', 'ever']\r\n","\r\n","\r\n","\r\n","# función auxiliar. Se utiliza al obtener la representación mediante TF-IDF del texto pues en este caso\r\n","# se removerán las stop_words y se considerarán los \"stem\" en lugar de las palabras.\r\n","def english_stemmer(sentence):\r\n","  stemmer = EnglishStemmer() \r\n","  analyzer = CountVectorizer(binary=False, analyzer='word', stop_words=stop_words, ngram_range=(1, 1)).build_analyzer() \r\n","  return (stemmer.stem(word) for word in analyzer(sentence))\r\n","\r\n","\r\n","# crear el pipeline\r\n","model01 = Pipeline([\r\n","            ('dataVect', CountVectorizer(analyzer=english_stemmer)),\r\n","            ('tfidf', TfidfTransformer(smooth_idf=True, use_idf=True)),\r\n","            ('classifier', SVC(probability=True))\r\n","          ])\r\n","\r\n","\r\n","# entrenar el modelo\r\n","model01.fit(train['Phrase'], train['Sentiment'])\r\n","\r\n","\r\n","print('Done!')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Done!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wErv-k5yvYyg"},"source":["#### Modelo 2: Transformer out-of-the-box sentiment analysis model.\r\n","\r\n","El segundo modelo considerado es un pipeline para la clasificación de textos incluido en la librería Transformers.\r\n","\r\n","Ejecute la siguiente casilla para instanciar el pipeline. Note que no es necesario entrenar pues este paso ya se ha realizado por Transformers."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GA4blGvEb6tB","executionInfo":{"status":"ok","timestamp":1615302886982,"user_tz":-60,"elapsed":1396,"user":{"displayName":"JOSE IGNACIO ABREU SALAS","photoUrl":"","userId":"06657112918169434086"}},"outputId":"c82d5421-c0c5-4243-9ef5-38dde1ba2354"},"source":["# configuraciones\r\n","cfg02 = {}\r\n","cfg02['framework'] = 'tf'\r\n","cfg02['task'] = 'sentiment-analysis'\r\n","cfg02['trained_model_name'] = 'distilbert-base-uncased-finetuned-sst-2-english'\r\n","cfg02['max_length'] = 512    # máxima longitud de secuencia recomendada por DistilBERT\r\n","cfg02['truncation'] = TruncationStrategy.ONLY_FIRST\r\n","\r\n","# cargar el tokenizador, disponible en Transformers. Establecer model_max_length para cuando el tokenizador sea llamado, trunque automáticamente.\r\n","cfg02['tokenizer'] = DistilBertTokenizer.from_pretrained(cfg02['trained_model_name'] , model_max_length=cfg02['max_length'])\r\n","\r\n","\r\n","# cargar el modelo, disponible en Transformers\r\n","cfg02['transformer'] = TFDistilBertForSequenceClassification.from_pretrained(cfg02['trained_model_name'])\r\n","cfg02['modelcard'] = ModelCard.from_pretrained(cfg02['trained_model_name'])\r\n","\r\n","# instanciar el pipeline para la clasificación de textos\r\n","model02 = TextClassificationPipeline(model=cfg02['transformer'], tokenizer=cfg02['tokenizer'], modelcard=None, framework=cfg02['framework'], task=cfg02['task'], return_all_scores=False)\r\n","\r\n","print('Done!')"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Done!\n"],"name":"stdout"},{"output_type":"stream","text":["Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n","- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_99']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["Done!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OUqm7EcKzLTJ"},"source":["#### Modelo 3: Fine-tunned Transformer (DistilBERT)\r\n","\r\n","El tercer y último clasificador será un modelo basado en Transformers, entrenado específicamente en nuestros datos.\r\n","\r\n","En este caso, es necesario tokenizar y convertir a tensores los datos de acuerdo a los requisitos de Transformers.\r\n","\r\n","Ejecute las siguientes casillas para preprocesar los datos, instanciar el modelo y entrenarlo. Preste atención a las explicaciónes dadas en los comentarios."]},{"cell_type":"code","metadata":{"id":"RmGoeEUk0LU-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615302053321,"user_tz":-60,"elapsed":8557,"user":{"displayName":"JOSE IGNACIO ABREU SALAS","photoUrl":"","userId":"06657112918169434086"}},"outputId":"70502a7e-ead4-4b16-d09a-233a938b526a"},"source":["# Preprocesamiento de los datos\r\n","\r\n","\r\n","# configuraciones\r\n","cfg03 = {} # diccinario para agrupar configuraciones y variables para su posterior uso\r\n","cfg03['framework'] = 'tf'    # TensorFlow como framework (por cuestiones del formato en los datos)\r\n","cfg03['max_length'] = 512    # máxima longitud de secuencia recomendada por DistilBERT\r\n","cfg03['trained_model_name'] = 'distilbert-base-uncased'\r\n","\r\n","\r\n","\r\n","# cargar el tokenizador, disponible en Transformers\r\n","cfg03['tokenizer'] = DistilBertTokenizer.from_pretrained(cfg03['trained_model_name'] )\r\n","tokenizer = cfg03['tokenizer']\r\n","\r\n","# obtener ids y máscaras para el conjunto de entrenamiento\r\n","# no es necesario convertir a tensores porque la salida del tokenizador se encuentra en este formato, \r\n","# ej. ver train_encodings['input_ids'] y train_encodings['attention_mask'] \r\n","train_encodings = tokenizer(train['Phrase'].to_list(), truncation=True, padding='max_length', max_length=cfg03['max_length'], return_tensors=cfg03['framework']) \r\n","\r\n","\r\n","# instanciar y entrenar LabelBinarizer\r\n","lb = preprocessing.LabelBinarizer()\r\n","lb.fit(train['Sentiment'])\r\n","num_labels = len(lb.classes_) # variable necesaria para la configuración del modelo, aunque al tratarse de clasificación binaria, se hará 1\r\n","cfg03['num_labels'] = 1\r\n","\r\n","# obtener codificación one-hot\r\n","train_blabels = lb.transform(train['Sentiment'])\r\n","\r\n","\r\n","# obtener tensores correspondientes\r\n","train_blabels_t = tf.convert_to_tensor(train_blabels, dtype='int32')\r\n","\r\n","print('Done!')"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Done!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0LJnRnXN1lRU"},"source":["Una vez preprocesados los datos, creamos e instanciamos el modelo."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zZXuW7d1yFr4","executionInfo":{"status":"ok","timestamp":1615302126703,"user_tz":-60,"elapsed":1130,"user":{"displayName":"JOSE IGNACIO ABREU SALAS","photoUrl":"","userId":"06657112918169434086"}},"outputId":"6fe91df3-e452-4a6e-ff47-898fab0c9702"},"source":["# configuraciones\r\n","config = DistilBertConfig(num_labels=cfg03['num_labels'], seq_classif_dropout=0.5)\r\n","\r\n","\r\n","# cargar el modelo pre-entrenado disponible en Transformers\r\n","model03 = TFDistilBertForSequenceClassification.from_pretrained(cfg03['trained_model_name'], config=config)\r\n","\r\n","\r\n","# finalizar configuración del modelo\r\n","# se sugiere revisar documentación para más detalles sobre los diferentes hiper-parámetros\r\n","optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\r\n","\r\n","\r\n","# definir función loss. Debe cuidarse que sea coherente con la salida esperada del modelo (vector de num_labels elementos)\r\n","# y el formato de los ejemplos (vector one-hot de num_labels componentes para codificar las categorías)\r\n","loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n","\r\n","\r\n","# compilar el modelo, indicando otras métricas que se desee monitorear\r\n","# La métrica debe ser apropiada para el tipo de problema (clasificación binaria o multiclase)\r\n","model03.compile(optimizer=optimizer, loss=loss, metrics=['binary_accuracy'])\r\n","\r\n","\r\n","# imprimir sumario del modelo\r\n","model03.summary()\r\n","\r\n","print('Done!')"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'vocab_transform', 'vocab_projector', 'activation_13']\n","- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'pre_classifier', 'dropout_79']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"tf_distil_bert_for_sequence_classification_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","distilbert (TFDistilBertMain multiple                  66362880  \n","_________________________________________________________________\n","pre_classifier (Dense)       multiple                  590592    \n","_________________________________________________________________\n","classifier (Dense)           multiple                  769       \n","_________________________________________________________________\n","dropout_79 (Dropout)         multiple                  0         \n","=================================================================\n","Total params: 66,954,241\n","Trainable params: 66,954,241\n","Non-trainable params: 0\n","_________________________________________________________________\n","Done!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9EDWuNTY2Qhv"},"source":["Y finalmente entrenamos el modelo."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ccC0L80e2Tji","executionInfo":{"status":"ok","timestamp":1615302759182,"user_tz":-60,"elapsed":595835,"user":{"displayName":"JOSE IGNACIO ABREU SALAS","photoUrl":"","userId":"06657112918169434086"}},"outputId":"bae8502e-79fe-4d35-cfeb-93d7ca3f7ba2"},"source":["# configuraciones\r\n","checkpoints_dir = 'checkpoints'\r\n","trained_model_name = os.path.join(checkpoints_dir, 'distilbert-review')\r\n","\r\n","epochs_max = 1\r\n","epochs_to_save = 1\r\n","batch_size = 16\r\n","\r\n","\r\n","# formatear los datos (tensores) de entrada de acuerdo a las opciones permitidas por TensorFlow \r\n","train_inputs = { 'input_ids': train_encodings['input_ids'],\r\n","            'attention_mask': train_encodings['attention_mask']\r\n","         }\r\n","\r\n","# ciclo de entrenamiento y guardar checkpoints\r\n","for epoch in tqdm(range(0, epochs_max, epochs_to_save)):\r\n","    print('Training model, epochs {0} - {1}'.format(epoch+1, epoch+epochs_to_save))\r\n","    \r\n","    # entrenar el modelo. Opcionalmente, se puede suministrar datos de validación => validation_data=(val_inputs,val_blabels_t )\r\n","    model03.fit(train_inputs, y=train_blabels_t, epochs=epochs_to_save, batch_size=batch_size)\r\n","\r\n","    #model03.save_pretrained(trained_model_name + '-epochs-{0:03d}-{1:03d}'.format(epoch+1, epoch+epochs_to_save))\r\n","    #tokenizer.save_pretrained(trained_model_name + '-epochs-{0:03d}-{1:03d}'.format(epoch+1, epoch+epochs_to_save))\r\n","\r\n","print('Done!')"],"execution_count":19,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/6 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training model, epochs 1 - 1\n","WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f01f3c34ec0>> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f01f3c34ec0>> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"],"name":"stdout"},{"output_type":"stream","text":["The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f020f4e4c20> and will run it as-is.\n","Cause: while/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"],"name":"stdout"},{"output_type":"stream","text":["The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING: AutoGraph could not transform <function wrap at 0x7f020f4e4c20> and will run it as-is.\n","Cause: while/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"],"name":"stdout"},{"output_type":"stream","text":["The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stderr"},{"output_type":"stream","text":["100/100 [==============================] - 123s 908ms/step - loss: 0.5680 - binary_accuracy: 0.6795\n"],"name":"stdout"},{"output_type":"stream","text":["\r 17%|█▋        | 1/6 [02:02<10:13, 122.67s/it]"],"name":"stderr"},{"output_type":"stream","text":["Training model, epochs 2 - 2\n","100/100 [==============================] - 94s 944ms/step - loss: 0.2251 - binary_accuracy: 0.9193\n"],"name":"stdout"},{"output_type":"stream","text":["\r 33%|███▎      | 2/6 [03:37<07:36, 114.23s/it]"],"name":"stderr"},{"output_type":"stream","text":["Training model, epochs 3 - 3\n","100/100 [==============================] - 94s 942ms/step - loss: 0.1429 - binary_accuracy: 0.9508\n"],"name":"stdout"},{"output_type":"stream","text":["\r 50%|█████     | 3/6 [05:11<05:24, 108.23s/it]"],"name":"stderr"},{"output_type":"stream","text":["Training model, epochs 4 - 4\n","100/100 [==============================] - 95s 945ms/step - loss: 0.0842 - binary_accuracy: 0.9735\n"],"name":"stdout"},{"output_type":"stream","text":["\r 67%|██████▋   | 4/6 [06:46<03:28, 104.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["Training model, epochs 5 - 5\n","100/100 [==============================] - 95s 946ms/step - loss: 0.0448 - binary_accuracy: 0.9868\n"],"name":"stdout"},{"output_type":"stream","text":["\r 83%|████████▎ | 5/6 [08:20<01:41, 101.29s/it]"],"name":"stderr"},{"output_type":"stream","text":["Training model, epochs 6 - 6\n","100/100 [==============================] - 95s 946ms/step - loss: 0.0450 - binary_accuracy: 0.9874\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 6/6 [09:55<00:00, 99.22s/it]"],"name":"stderr"},{"output_type":"stream","text":["Done!\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"_eikG35w4IQC"},"source":["### Construcción del ensemble\r\n","\r\n","Como el stack de clasificadores se construirá de forma manual, es preciso realizar los siguientes pasos:\r\n","\r\n","- utilizar los modelos base para predecir las instancias del conjunto de entrenamiento. En este caso en lugar de la categoría, recuperaremos la probabilidad asignada por el clasificador a que la instancia sea positiva.\r\n","\r\n","- combinar las predicciones de cada modelo para obtener el conjunto de entrenamiento del metaclasificador.\r\n","\r\n","- entrenar el metaclasificador"]},{"cell_type":"markdown","metadata":{"id":"3B5eThVb4PDa"},"source":["#### Obtener predicciones de los modelos base\r\n","\r\n","Para entrenar el metaclasificador, utilizaremos la partición **train** previamente creada. Recordemos que hemos reservado la partición **eval** para evaluar cada modelo y compararlo respecto al  metaclasificador.\r\n","\r\n","Ejecute las siguientes casillas para obtener las predicciones de los modelos base."]},{"cell_type":"code","metadata":{"id":"9MdwiZ934Ng7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615306102903,"user_tz":-60,"elapsed":976,"user":{"displayName":"JOSE IGNACIO ABREU SALAS","photoUrl":"","userId":"06657112918169434086"}},"outputId":"a33603e4-8fb3-4294-a88b-d50495dd69e8"},"source":["# predicciones modelo 1\r\n","m01_pscores = model01.predict_proba(train['Phrase'])[:,1]\r\n","print('Done!')"],"execution_count":68,"outputs":[{"output_type":"stream","text":["Done!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ABMuwjfJ4ivo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615306249810,"user_tz":-60,"elapsed":140666,"user":{"displayName":"JOSE IGNACIO ABREU SALAS","photoUrl":"","userId":"06657112918169434086"}},"outputId":"6b77df82-f1ce-49aa-94c2-c4d85f5ff775"},"source":["# predicciones modelo 2\r\n","# configuraciones\r\n","batch_size = 128 # puede indicar un valor menor para disminuir el consumo de memoria \r\n","size = train.shape[0]\r\n","\r\n","\r\n","# predecir los datos de entrenamiento\r\n","m02_pred = []\r\n","m02_pscores = []\r\n","for i in tqdm(range(0, size, batch_size)):\r\n","    batch_text = train['Phrase'][i:i+batch_size].to_list()\r\n","    results = model02(batch_text, truncation=cfg02['truncation'])\r\n","    for pred in results:\r\n","      m02_pred.append(pred['label'].lower())\r\n","      m02_pscores.append(pred['score'] if pred['label']=='POSITIVE' else 1-pred['score'])\r\n","    \r\n","m02_pscores = np.asarray(m02_pscores)\r\n","\r\n","print('Done!')"],"execution_count":69,"outputs":[{"output_type":"stream","text":["100%|██████████| 2/2 [02:20<00:00, 70.07s/it]"],"name":"stderr"},{"output_type":"stream","text":["Done!\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"ESigiy-_2fZb","executionInfo":{"status":"ok","timestamp":1615306256798,"user_tz":-60,"elapsed":4932,"user":{"displayName":"JOSE IGNACIO ABREU SALAS","photoUrl":"","userId":"06657112918169434086"}}},"source":["# predicciones modelo 3\r\n","\r\n","# tokenizar\r\n","tokenizer = cfg03['tokenizer']\r\n","encodings = tokenizer(train['Phrase'].to_list(), truncation=True, padding='max_length', max_length=cfg03['max_length'], return_tensors=cfg03['framework'])\r\n","\r\n","\r\n","inputs = {'input_ids': encodings['input_ids'],\r\n","          'attention_mask': encodings['attention_mask'],\r\n","         }\r\n","\r\n","\r\n","# predecir los datos de prueba\r\n","m03_pscores = model03.predict(inputs)['logits'][:,0]\r\n","\r\n","print('Done!')"],"execution_count":70,"outputs":[]},{"cell_type":"code","metadata":{"id":"e9Lp6AnY4ozR","colab":{"base_uri":"https://localhost:8080/","height":408},"executionInfo":{"status":"error","timestamp":1615307522554,"user_tz":-60,"elapsed":636,"user":{"displayName":"JOSE IGNACIO ABREU SALAS","photoUrl":"","userId":"06657112918169434086"}},"outputId":"df6967fd-7f5c-4986-fbf9-46be7ae0f7e0"},"source":["# combinar predicciones ()\r\n","train_cmb =  pd.DataFrame({'m01_scores': m01_pscores, 'm02_scores': m02_pscores, 'm03_scores':m03_pscores, 'Sentiment':train['Sentiment']})\r\n","\r\n","\r\n","# separar entradas y salidas esperadas para satisfacer formato requerido por scikit-learn\r\n","X = train_cmb.loc[:,['m01_scores', 'm02_scores', 'm03_scores']].values\r\n","y = train_cmb['Sentiment']\r\n","\r\n","print(train_cmb.head(5))\r\n","print('Done!')"],"execution_count":81,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-81-ea4e38efc085>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# combinar predicciones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_cmb\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'm01_scores'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mm01_pscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'm02_scores'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mm02_pscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'm03_scores'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mm03_pscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cmb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         ]\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    409\u001b[0m                         \u001b[0;34mf\"length {len(index)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                     )\n\u001b[0;32m--> 411\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mibase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: array length 177 does not match index length 1586"]}]},{"cell_type":"markdown","metadata":{"id":"CGXI29fGwWMJ"},"source":["#### Instanciar y entrenar metaclasificador\r\n","\r\n","Finalmente, podemos utilizar el nuevo conjunto de entrenamiento formado por la combinación de los predictores base para entrenar el metaclasificador. En este caso, utilizaremos la implementación de árboles de decisión en "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LDdO9UUHwZ-4","executionInfo":{"status":"ok","timestamp":1615307252080,"user_tz":-60,"elapsed":1612,"user":{"displayName":"JOSE IGNACIO ABREU SALAS","photoUrl":"","userId":"06657112918169434086"}},"outputId":"cbd51fa7-6461-49e4-cfba-4442a8849cb4"},"source":["from sklearn.tree import DecisionTreeClassifier\r\n","\r\n","\r\n","\r\n","\r\n","# instanciar el clasificador\r\n","classifier = DecisionTreeClassifier(random_state=seed)\r\n","\r\n","\r\n","# entrenar el clasificador\r\n","classifier.fit(X, y)"],"execution_count":80,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['positive', 'positive', 'negative', 'positive', 'negative',\n","       'negative', 'positive', 'positive', 'negative', 'negative',\n","       'negative', 'negative', 'positive', 'positive', 'negative',\n","       'positive', 'negative', 'positive', 'positive', 'positive',\n","       'positive', 'negative', 'positive', 'positive', 'positive',\n","       'positive', 'negative', 'negative', 'positive', 'positive',\n","       'negative', 'negative', 'positive', 'negative', 'negative',\n","       'negative', 'negative', 'negative', 'negative', 'negative',\n","       'positive', 'positive', 'positive', 'negative', 'positive',\n","       'positive', 'positive', 'positive', 'positive', 'negative',\n","       'positive', 'negative', 'positive', 'negative', 'positive',\n","       'negative', 'negative', 'negative', 'negative', 'negative',\n","       'positive', 'negative', 'positive', 'negative', 'positive',\n","       'negative', 'positive', 'positive', 'negative', 'positive',\n","       'positive', 'negative', 'positive', 'positive', 'negative',\n","       'negative', 'negative', 'positive', 'positive', 'negative',\n","       'negative', 'positive', 'positive', 'negative', 'positive',\n","       'positive', 'negative', 'negative', 'positive', 'negative',\n","       'positive', 'positive', 'positive', 'negative', 'positive',\n","       'negative', 'negative', 'positive', 'positive', 'positive',\n","       'negative', 'negative', 'negative', 'positive', 'positive',\n","       'positive', 'negative', 'negative', 'negative', 'negative',\n","       'positive', 'negative', 'negative', 'negative', 'positive',\n","       'positive', 'negative', 'positive', 'negative', 'negative',\n","       'positive', 'positive', 'negative', 'positive', 'positive',\n","       'negative', 'negative', 'negative', 'positive', 'positive',\n","       'negative', 'negative', 'negative', 'negative', 'positive',\n","       'negative', 'positive', 'negative', 'positive', 'positive',\n","       'positive', 'positive', 'positive', 'negative', 'negative',\n","       'negative', 'positive', 'negative', 'negative', 'positive',\n","       'negative', 'positive', 'positive', 'positive', 'negative',\n","       'negative', 'positive', 'positive', 'negative', 'negative',\n","       'negative', 'positive', 'negative', 'positive', 'positive',\n","       'positive', 'positive', 'negative', 'negative', 'positive',\n","       'negative', 'negative', 'negative', 'positive', 'negative',\n","       'negative', 'negative'], dtype=object)"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"markdown","metadata":{"id":"Av0dczu-2RKm"},"source":["### Comparando resultados\r\n","\r\n","Finalmente, realizaremos una comparación entre los resultados que alcanzan los modelos base y el ensemble. Para esto, utilizaremos la porción **val**."]},{"cell_type":"markdown","metadata":{"id":"NkMQS-mt2pS7"},"source":["#### Evaluación Modelo 1"]},{"cell_type":"markdown","metadata":{"id":"ZoD0MhI42wy6"},"source":["#### Evaluación Modelo 2"]},{"cell_type":"markdown","metadata":{"id":"p8MJMQZf21Ar"},"source":["#### Evaluación Modelo 3"]},{"cell_type":"markdown","metadata":{"id":"2TDuA0Ww22cq"},"source":["#### Evaluación Ensemble"]},{"cell_type":"code","metadata":{"id":"XTXb7dBb2wWC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ujnfNv3fwtpP"},"source":[""],"execution_count":null,"outputs":[]}]}